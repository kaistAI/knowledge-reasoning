# Open source LLM inference parameters
SAMPLING_PARAMS = {
    "max_tokens": 1024,
    "temperature": 1.0,
    "top_p": 0.9,
    "repetition_penalty": 1.03,
}

# OpenAI inference/evaluation parameters
SAMPLING_PARAMS_OPENAI = {"max_tokens": 1024, "temperature": 1.0, "top_p": 0.9}
